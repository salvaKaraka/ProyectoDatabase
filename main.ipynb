{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da0097a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas sqlalchemy langchain langchain_community langchain_openai python_dotenv  --default-timeout=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4f00d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./F1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee7d792",
   "metadata": {},
   "source": [
    "# Creaci√≥n de base de datos SQL a partir de dataframe (csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3217f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine('sqlite:///f1.db', echo=True)\n",
    "df.to_sql('f1', con=engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0954cd",
   "metadata": {},
   "source": [
    "# Creaci√≥n de chain y agente\n",
    "Utilizaremos una LLM chain para verificar la validez del prompt y para que ayude a reformularlo\n",
    "Utilizaremos un agente especializado para generar las consultas sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ba7a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "#from langchain_openai import ChatOpenAI #OpenAI LLM\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI #Google LLM\n",
    "\n",
    "\n",
    "# -------- Chain de LLM para validar y reformular prompts ---------\n",
    "# from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "prompt_template = \"\"\"Eres un experto en SQL y en la base de datos de F1.\n",
    "Tu tarea es verificar la validez de un prompt dado por un usuario y, si es necesario, reformularlo para que sea m√°s claro y espec√≠fico.\n",
    "Si el prompt es v√°lido, simplemente devu√©lvelo tal cual.\n",
    "- Si la consulta original es ambigua o puede interpretarse de m√°s de una forma, hac√© preguntas aclaratorias.\n",
    "- Si la consulta es clara y no necesita aclaraci√≥n, devolv√© exactamente: NO_CLARIFICATION_NEEDED\n",
    "\n",
    "Ejemplo:\n",
    "Usuario: ¬øCu√°ntos atendi√≥ Juan?\n",
    "Asistente:\n",
    "1. ¬øA qu√© se refiere con \"atendi√≥\"? (consultas, estudios, turnos, etc.)\n",
    "2. ¬øQui√©n es \"Juan\"? ¬øTen√©s apellido o rol (m√©dico, paciente)?\n",
    "3. ¬øQuer√©s filtrar por fechas?\n",
    "\n",
    "Ejemplo de pregunta clara:\n",
    "Usuario: ¬øCu√°ntos pacientes atendi√≥ Juan P√©rez en 2023?\n",
    "Asistente: NO_CLARIFICATION_NEEDED\n",
    "\n",
    "Gener√° preguntas cortas y claras para que el usuario aclare su intenci√≥n, una por l√≠nea. No respondas la consulta.\n",
    "\n",
    "Usuario: la siguiente consulta puede ser ambigua: \"{pregunta}\"\n",
    "Asistente:\n",
    "\"\"\"\n",
    "clasificador_prompt = PromptTemplate(\n",
    "    input_variables=[\"pregunta\"],\n",
    "    template = prompt_template,\n",
    ")\n",
    "# llm = ChatOpenAI(model=\"gpt-4o\", temperature=0, openai_api_key=os.getenv(\"OPENAI_API_KEY\")) #openai llm\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.2, google_api_key=os.getenv(\"GOOGLE_API_KEY\")) #google llm\n",
    "\n",
    "clarificador_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=clasificador_prompt,\n",
    "    verbose=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471b1d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import SQLDatabase\n",
    "from langchain_community.agent_toolkits import create_sql_agent\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "db = SQLDatabase(engine=engine)\n",
    "# --------- Agente SQL con LLM ---------\n",
    "# llm = ChatOpenAI(model=\"gpt-4o\", temperature=0, openai_api_key=os.getenv(\"OPENAI_API_KEY\")) #openai llm\n",
    "#agente = create_sql_agent(llm=llm, database=db,agent_type=\"openai-tools\" , verbose=True) #verbose=True para ver como \"piensa\" el agente\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.2, google_api_key=os.getenv(\"GOOGLE_API_KEY\")) #google llm\n",
    "#Al parecer gemini-2.5-pro es mas lenta que gemini-2.0-flash y mucho mas lenta que 2.5-flash\n",
    "agente = create_sql_agent(llm=llm, db=db, agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True, handle_parsing_errors=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab45251f",
   "metadata": {},
   "source": [
    "# LLM chain que explica las consultas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b1a3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- LLM Chain para explicar la consulta SQL generada ---------\n",
    "# 6. Chain explicador\n",
    "template_explicador = PromptTemplate.from_template(\"\"\"\n",
    "Ten√©s que explicarle al usuario un resultado de una consulta SQL que pidi√≥ en lenguaje natural.\n",
    "\n",
    "Pregunta original:\n",
    "\"{pregunta}\"\n",
    "\n",
    "Aclaraciones:\n",
    "{aclaraciones}\n",
    "\n",
    "Resultado de la consulta SQL:\n",
    "\"{resultado}\"\n",
    "\n",
    "Respond√© con una frase como:\n",
    "\"La respuesta es: ...\" y luego explic√° en lenguaje claro el significado de ese resultado, como si se lo explicaras a alguien sin conocimientos t√©cnicos.\n",
    "\"\"\")\n",
    "\n",
    "explicador_chain = LLMChain(llm=llm, prompt=template_explicador)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2579f4cd",
   "metadata": {},
   "source": [
    "# LLM chain que clasifica si una respuesta fue o no util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec015a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Sos un asistente que clasifica si una explicaci√≥n fue √∫til para el usuario.\n",
    "\n",
    "Respuesta del usuario:\n",
    "\"{respuesta_usuario}\"\n",
    "\n",
    "Clasific√° esta respuesta como una de las siguientes opciones (solo una palabra):\n",
    "- √∫til\n",
    "- no √∫til\n",
    "\"\"\"\n",
    "\n",
    "clasificador_prompt = PromptTemplate.from_template(template)\n",
    "clasificador_chain = LLMChain(prompt=clasificador_prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4ac095",
   "metadata": {},
   "source": [
    "# LLM chain que reformula la pregunta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ee6a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Ten√©s una conversaci√≥n previa con el usuario, en la que se intent√≥ responder una pregunta en lenguaje natural transform√°ndola en SQL. A continuaci√≥n se incluye el historial y un comentario final del usuario.\n",
    "\n",
    "Historial:\n",
    "{historial}\n",
    "\n",
    "Nueva aclaraci√≥n o correcci√≥n del usuario:\n",
    "{nueva_aclaracion}\n",
    "\n",
    "Pregunta original:\n",
    "{pregunta_original}\n",
    "\n",
    "Reformul√° una nueva pregunta clara, espec√≠fica y completa en lenguaje natural que tenga en cuenta todo el contexto y la aclaraci√≥n.\n",
    "Solo devolv√© la nueva pregunta, sin explicaciones adicionales.\n",
    "\"\"\"\n",
    "\n",
    "reformulador_prompt = PromptTemplate.from_template(template)\n",
    "reformulador_chain = LLMChain(prompt=reformulador_prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f6a0cf",
   "metadata": {},
   "source": [
    "# Funcion que maneja las consultas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f069019f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_consulta_sql(pregunta_usuario: str, clarificador_chain, sql_agent, explicador_chain, clasificador_chain, reformulador_chain, max_intentos=3):\n",
    "    historial = []\n",
    "    respuestas_usuario = {}\n",
    "    prompt_actual = pregunta_usuario\n",
    "    intentos = 0\n",
    "    aclaraciones_str = \"\"\n",
    "\n",
    "    while intentos < max_intentos:\n",
    "        print(f\"\\nüîÑ Iteraci√≥n #{intentos + 1} - Refinando la pregunta...\\n\")\n",
    "\n",
    "        # Paso 1: Clarificaci√≥n guiada\n",
    "        preguntas = clarificador_chain.run({\"pregunta\": prompt_actual}).strip()\n",
    "\n",
    "        if preguntas == \"NO_CLARIFICATION_NEEDED\":\n",
    "            print(\"‚úÖ No hace falta pedir m√°s aclaraciones.\")\n",
    "            prompt_claro = pregunta_usuario\n",
    "        else:\n",
    "            nuevas_respuestas = {}\n",
    "            for pregunta in preguntas.split(\"\\n\"):\n",
    "                if pregunta.strip():\n",
    "                    user_input = input(f\"{pregunta.strip()} üëâ \")\n",
    "                    nuevas_respuestas[pregunta.strip()] = user_input\n",
    "\n",
    "            respuestas_usuario.update(nuevas_respuestas)\n",
    "\n",
    "            aclaraciones_str = \"\\n\".join(f\"- {k}: {v}\" for k, v in respuestas_usuario.items())\n",
    "            prompt_claro = f\"\"\"Pregunta original: {pregunta_usuario}\n",
    "        Aclaraciones:\n",
    "        {aclaraciones_str}\"\"\"\n",
    "\n",
    "        print(\"\\nü§ñ Ejecutando consulta...\\n\")\n",
    "        resultado = sql_agent.run(prompt_claro)\n",
    "\n",
    "        if \"error\" in resultado.lower() or resultado.strip() == \"\":\n",
    "            print(\"‚ö†Ô∏è La consulta no fue exitosa. Vamos a pedir m√°s detalles...\")\n",
    "            prompt_actual = prompt_claro\n",
    "            intentos += 1\n",
    "            continue\n",
    "\n",
    "        # Paso 2: Explicar el resultado\n",
    "        explicacion = explicador_chain.run({\n",
    "            \"pregunta\": pregunta_usuario,\n",
    "            \"aclaraciones\": aclaraciones_str,\n",
    "            \"resultado\": resultado\n",
    "        })\n",
    "\n",
    "        print(\"\\n\"+ explicacion +\"\\nüß† Explicaci√≥n final para el usuario:\\n\")\n",
    "        print(explicacion)\n",
    "\n",
    "        # Paso 3: Feedback\n",
    "        feedback = input(\"\\n‚úçÔ∏è ¬øTe result√≥ √∫til esta explicaci√≥n? Pod√©s responder con una frase üëâ \").strip()\n",
    "        clasificacion = clasificador_chain.run({\n",
    "            \"respuesta_usuario\": feedback\n",
    "        }).strip().lower()\n",
    "\n",
    "        historial.append({\n",
    "            \"pregunta\": pregunta_usuario,\n",
    "            \"aclaraciones\": respuestas_usuario.copy(),\n",
    "            \"prompt_final\": prompt_claro,\n",
    "            \"resultado_sql\": resultado,\n",
    "            \"explicacion\": explicacion,\n",
    "            \"feedback_usuario\": feedback,\n",
    "            \"clasificacion_feedback\": clasificacion,\n",
    "        })\n",
    "\n",
    "        if \"√∫til\" == clasificacion:\n",
    "            print(\"‚úÖ ¬°Gracias! Me alegra que te haya servido.\")\n",
    "            return resultado, historial\n",
    "\n",
    "        # Paso 4: Reformulaci√≥n si no fue √∫til\n",
    "        print(\"üîÅ Gracias por tu comentario. Vamos a intentar mejorar la consulta...\")\n",
    "\n",
    "        contexto_historial = \"\"\n",
    "        for h in historial:\n",
    "            contexto_historial += f\"\"\"\n",
    "[Pregunta anterior]: {h['pregunta']}\n",
    "[Aclaraciones]: {h['aclaraciones']}\n",
    "[Respuesta SQL]: {h['resultado_sql']}\n",
    "[Explicaci√≥n]: {h['explicacion']}\n",
    "[Feedback]: {h['feedback_usuario']}\n",
    "\"\"\"\n",
    "\n",
    "        nueva_pregunta = reformulador_chain.run({\n",
    "            \"historial\": contexto_historial,\n",
    "            \"nueva_aclaracion\": feedback,\n",
    "            \"pregunta_original\": pregunta_usuario\n",
    "        }).strip()\n",
    "\n",
    "        print(f\"\\nüìå Reformulando la pregunta como:\\n{nueva_pregunta}\\n\")\n",
    "        prompt_actual = nueva_pregunta\n",
    "        intentos += 1\n",
    "\n",
    "    print(\"\\n‚ùå No pudimos entender bien tu consulta despu√©s de varios intentos.\")\n",
    "    return None, historial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2ae4b4",
   "metadata": {},
   "source": [
    "# Entrada y salida via consola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acffd459",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    pregunta = input(\"üßë‚Äçüíª ¬øQu√© consulta quer√©s hacer? üëâ \")\n",
    "    resultado, historial = loop_consulta_sql(pregunta, clarificador_chain, agente, explicador_chain, clasificador_chain, reformulador_chain)\n",
    "    print(\"\\nüìä Resultado de la consulta SQL:\" )\n",
    "    if resultado:\n",
    "        print(resultado)\n",
    "        print(\"explicacion:\", historial[-1]['explicacion'])\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No se pudo obtener un resultado v√°lido.\")\n",
    "\n",
    "    print(\"\\nüìú Historial de la sesi√≥n:\")\n",
    "    for paso in historial:\n",
    "        print(paso)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
